P1-P4, P6, P7, P9-P11

Introduction of Machine / Deep Learning

---
### 什么是机器学习？
机器学习就是让机器找一个函数，根据输入，这个函数会输出要得到的结果

![img1](image/img1.png)

|  应用场景举例   | 函数输入  | 函数输出  |
|  ----  | ----  | ----  |
| 语音识别 | 音频 | 文字 |
| 图像识别  | 图片 | 类别 |
| Playing Go  | 当前棋局 | 下一步位置 |


### 机器学习的类别
1. Regression（回归） 
   
         若预测的是连续值，则称为回归。

2. Classification（分类）

         若预测的为离散值，则称为分类。

   让机器做选择题。从设定好的选项里（也就是类别），选择一个当作输出。

3. Structured Learning（结构化学习）
      
         机器在学习的时候不只输出一个数字，不单单做选择题，还要生成有结构的物件。
   也就是让机器学会创造。

**输入输出种类**
1. 输入
   - vector 向量 
   - Matrix 矩阵
   - Sequence 序列
2. 输出
   - regression (数值)
   - classification (分类)
   - text/image

### 机器学习寻找Function的的三个步骤
1. Function with Unknown Parameters（未知参数函数）
               
         写出带有未知参数的函数
假设y=b+w*x，其中b与w是未知的，称为参数（parameter）；而这个带有Unknown的Parameter的Function，称之为Model（模型）

x在这个Function中是已知的，是前一天的后台数据，称之为Feature（特征）；w是与Feature相乘的数，称之为weight（权重）；b没有与feature相乘，称之为Bias（偏离率）。
2. Define Loss from Training Data（定义训练数据的损失函数）

         定义一个Loss（损失函数），Loss也是一个Function，其输入为Model的参数，输出代表了当前输入的参数的值的好坏

定义Loss函数为L(b,w)，损失函数是关于模型中的未知参数的函数，损失函数表示模型预测值和真实值之间的差距。Loss函数可以选择为平均绝对值误差、平均平方误差、交叉熵（y是概率分布时）等，根据对具体任务的要求进行选择。Loss函数值越小表明参数越好。

3. Optimization（最优化）

         找出使得Loss函数最小的未知参数
一般来说，用到的方法就是**Gradient Descent（梯度下降法）**

为了方便，先假设参数只有一个w。

当w的值不同时，对应的Loss的值也不同，这些Loss组成了error surface，由于参数只有一个，则误差曲面也就成为了一维的。

**寻找w的方法**

1.随机选一个初始的点，wº。（不一定完全随机，有选择的方法）

2.计算w=wº时，w对loss的微分是多少，也就是在wº上误差曲面的切线斜率

3.如果误差曲线的左边比较高右边比较低的话，就把w的值变大，就可以让loss变小。如果算出来的斜率是正的，就代表说左边比较低右边比较高，w往左边移，可以让Loss的值变小。

移动w的幅度大小取决于两件事：

1.这个地方的斜率有多大。斜率大，步伐就跨大一点，斜率小步伐就跨小一点。

2.Learning rate（学习速率），令学习速率为η，η是人手动设置的。η的值越大，参数每次update的量就会越大；如果η的值很小，则参数update就会很慢，每次只会改一点点参数的值。（这类需要人手工设置的参数叫做hyperparameters 超参数）

停下来的情况：

1.第一种状况是你自己决定的,做一个deadline，手动决定需要更新几次（超参数）

2.另外一种理想情况，即参数在调整的过程中算出来的微分正好为0时（也就是该点的切线为水平的），参数不会再移动位置了。
![img2.png](image/img2.png)
但是在梯度下降的这个方法中会遇到global minima（全局最小）的情况，也会遇到local minima（局部最小）的情况，局部最小点的左右两边都比该点的loss还要高一点，但是它不是整个error surface上面的最低点。

以上的例子时参数只有一个的操作流程，可以推广到多个参数。
![img3.png](image/img3.png)

---
